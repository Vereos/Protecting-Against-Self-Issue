{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"private_outputs":true,"authorship_tag":"ABX9TyMl7JDcvAALycZeq61vihuF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"T1LcVQj5T9n7"},"source":["# Practical Protection of Voice-Controllable Devices Against Self-Issued Voice Commands\n","\n","This is the source code for the solution against voice command self-issue presented in the paper *Practical Protection of Voice-Controllable Devices Against Self-Issued Voice Commands*."]},{"cell_type":"markdown","source":["# Instructions\n","\n","### Before You Start\n","\n","Before running this code, please put the `augmented-dataset.zip` archive in the `/content/` folder. Please note that this folder is the one already opened in the left panel of Google Colab after booting up, i.e., the same directory in which you can find the `sample_data/` folder. The code in this notebook will automatically extract the dataset.\n","\n","When you first run this on Google Colab, the first instruction upgrades the Pillow package. At some point, the execution will halt with an error similar to:\n","\n","```\n","ImportError: cannot import name 'is_directory' from 'PIL._util' (...)\n","```\n","\n","**This is normal!**\n","\n","Just click `Runtime > Restart and Execute All` on top of the screen to restart the environment and everything should work correctly.\n","\n","### Performing the Preprocessing\n","\n","The augmented dataset is obtained through the iteration of the `preprocess()` function defined below, for all samples. Because the augmented samples are already available for download, the lines that call the `preprocess()` function are not executed. However, if you want to perform the preprocessing yourself, which will give the exact same files except for augmentation 5 that is random, you will need to:\n","\n","- Set the `preprocessing_from_scratch` flag to `True`\n","- Put the `selfissue-dataset-public-v1.zip` archive in the `/content/` folder of your Google Colab machine. This notebook will extract it automatically.\n","- Because the augmentation will not work properly if the directory tree for the augmented files is not present, the `augmented-dataset-structure.zip` archive must be used, so that all directories and the .csv files already exist. Place the archive in the `/content/` folder as usual. This notebook will extract it automatically **only** if you set the `preprocessing_from_scratch` flag to `True`.\n","\n","### Running this outside of Google Colab (untested)\n","\n","It should work as long as you have CUDA support. Install the following requirements (other versions of these packages might work as well):\n","\n","- Librosa 0.8.1 (0.8.0 and 0.9.2 should work as well)\n","- Matplotlib 3.2.2 (3.0.2 and 3.6.0 should work as well)\n","- Numba 0.56.3 (0.49.0 and 0.56.2 should work as well)\n","- Numpy 1.21.6 (1.23.3 should work as well) \n","- Pandas 1.3.5 (1.5.0 should work as well)\n","- Pillow 9.2.0\n","- Resampy 0.4.2 (0.2.2 should work as well)\n","- Scikit-Image 0.18.3 (0.19.3 should work as well)\n","- Scikit-Learn 1.0.2 (1.1.2 should work as well)\n","- Torch 1.12.1+cu113\n","- Torchvision 0.13.1+cu113"],"metadata":{"id":"wZ7FOyKrNfek"}},{"cell_type":"code","source":["!sudo pip3 install --upgrade pillow"],"metadata":{"id":"Mj83fnPffoKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change this value to True if you want to perform the preprocessing again.\n","preprocessing_from_scratch = False"],"metadata":{"id":"S2Bt-0aXKijo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Extracts the Dataset Structure only\n","# Is only executed if the preprocessing flag was manually set to True\n","if preprocessing_from_scratch:\n","  if os.path.exists(\"/content/augmented-dataset/\") == False:\n","    if os.path.exists(\"/content/augmented-dataset-structure.zip\") == True:\n","      print(\"Extracting Augmented Dataset Structure...\")\n","      !unzip \"/content/augmented-dataset-structure.zip\" -d \"/content/\"\n","      print(\"Done!\")\n","    else:\n","      print(\"Augmented Dataset Structure file not found. Skipping unzip.\")\n","  else:\n","    print(\"Augmented Dataset Structure already extracted. Skipping unzip.\")\n","\n","# Extracts the Augmented Dataset\n","if os.path.exists(\"/content/augmented-dataset/\") == False:\n","  if os.path.exists(\"/content/augmented-dataset.zip\") == True:\n","    print(\"Extracting Augmented Dataset...\")\n","    !unzip \"/content/augmented-dataset.zip\" -d \"/content/\"\n","    print(\"Done!\")\n","  else:\n","    print(\"Augmented Dataset file not found. Skipping unzip.\")\n","else:\n","  print(\"Augmented Dataset already extracted. Skipping unzip.\")\n","\n","# Extracts the Self-Issue Dataset (not the augmented one)\n","if os.path.exists(\"/content/selfissue-dataset-public-v1/\") == False:\n","  if os.path.exists(\"/content/selfissue-dataset-public-v1.zip\") == True:\n","    print(\"Extracting Self-Issue Dataset...\")\n","    !unzip \"/content/selfissue-dataset-public-v1.zip\" -d \"/content/\"\n","    print(\"Done!\")\n","  else:\n","    print(\"Self-Issue Dataset file not found. Skipping unzip.\")\n","else:\n","  print(\"Self-Issue Dataset already extracted. Skipping unzip.\")"],"metadata":{"id":"OGNGGrPmt7mW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing and Augmentation Stage\n","\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import os\n","import skimage.io\n","import random\n","\n","base_path = \"/content/augmented-dataset/\"\n","\n","def scale_minmax(X, min=0.0, max=1.0):\n","  X_std = (X - X.min()) / (X.max() - X.min())\n","  X_scaled = X_std * (max - min) + min\n","  return X_scaled\n","\n","def trim(img, offset=0):\n","  return img[:,0+offset:]\n","\n","def preprocess(audiofile, num, mask1, mask2):\n","  y, sr = librosa.load(audiofile, sr=None) # Extract samples (y) and sample rate (sr)\n","  img = generate_mel_spectrogram(y, sr, num)\n","  audiofilename = audiofile[len(\"/content/selfissue-dataset-public-v1/\"):] # Extracts the file name with its path, e.g. /malicious/dw/rec/5-rec.wav\n","  finalName = os.path.join(\"/content/augmented-dataset/\", audiofilename + \".png\") # Generates the final path of the preprocessed sample, e.g. /content/augmented-dataset/malicious/dw/rec/5-rec.wav.png\n","  print(\"Saving: \" + finalName)\n","  skimage.io.imsave(finalName, img)\n","  # Augmentation Starts Here\n","  # Do not touch img as we need it for the masking augmentation process\n","  # 1. Pitch Increase\n","  pitchPlusY = augmentation_pitch(y, sr, 2)\n","  skimage.io.imsave(finalName+\".aug1.png\", generate_mel_spectrogram(pitchPlusY, sr, num))\n","  print(\"Augmentation 1 OK.\")\n","  # 2. Pitch Decrease\n","  pitchMinusY = augmentation_pitch(y, sr, -2)\n","  skimage.io.imsave(finalName+\".aug2.png\", generate_mel_spectrogram(pitchMinusY, sr, num))\n","  print(\"Augmentation 2 OK.\")\n","  # 3. Speed Increase\n","  speedPlusY = augmentation_speed(y, 1.2)\n","  skimage.io.imsave(finalName+\".aug3.png\", generate_mel_spectrogram(speedPlusY, sr, num))\n","  print(\"Augmentation 3 OK.\")\n","  # 4. Speed Decrease\n","  speedMinusY = augmentation_speed(y, 0.8)\n","  skimage.io.imsave(finalName+\".aug4.png\", generate_mel_spectrogram(speedMinusY, sr, num))\n","  print(\"Augmentation 4 OK.\")\n","  # 5. Frequency Mask (Horizontal)\n","  maskImg = augmentation_mask_frequency(img, mask1, mask2)\n","  skimage.io.imsave(finalName+\".aug5.png\", maskImg)\n","  print(\"Augmentation 5 OK.\")\n","  \n","def generate_mel_spectrogram(y, sr, num):\n","  mel = librosa.feature.melspectrogram(y=y, sr=sr) # Extract Mel Spectrogram (Mel x Amplitude x Time)\n","  mel_db = librosa.power_to_db(mel, ref=np.max) # Convert Amplitude to Db, to create another Mel Spectrogram (Mel x Db x Time)\n","  img = scale_minmax(mel_db, 0, 255).astype(np.uint8)\n","  img = np.flip(img, axis=0)\n","  img = 255-img\n","  if (num == 1):\n","    img = trim(img, 0)\n","  else:\n","    img = trim(img, 20)\n","  return img\n","\n","def augmentation_pitch(data, sampling_rate, pitch_factor):\n","    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n","\n","def augmentation_speed(data, speed_factor):\n","    return librosa.effects.time_stretch(data, speed_factor)\n","\n","def augmentation_mask_frequency(maskImg, mask1, mask2): #6 pixel x 2 masks\n","  maskImg[mask1:mask1+5,:] = (maskImg[mask1:mask1+5,:] * 0) + 255\n","  maskImg[mask2:mask2+5,:] = (maskImg[mask2:mask2+5,:] * 0) + 255\n","  return maskImg\n","\n","# Is only executed if the preprocessing flag was manually set to True\n","if preprocessing_from_scratch:\n","  # Preprocess Training Samples\n","  dataToProcess = pd.read_csv(\"/content/selfissue-dataset-public-v1/training-example-wav.csv\", header=None)\n","  for i, elem in dataToProcess.iterrows():\n","    audio1_path = os.path.join(\"/content/selfissue-dataset-public-v1/\", elem[0]) # elem[0] = played audio || elem[1] = recorded audio || elem[2] = label\n","    audio2_path = os.path.join(\"/content/selfissue-dataset-public-v1/\", elem[1])\n","    random.seed()\n","    mask1 = random.randrange(123) # so the upper limit is 122, in case it gets picked the 6px wide mask will hit rows 122, 123, 124, 125, 126 and 127.\n","    mask2 = random.randrange(123)\n","    preprocess(audio1_path, 1, mask1, mask2)\n","    preprocess(audio2_path, 2, mask1, mask2)\n","  # Preprocess Testing Samples -- Augmented Samples will be discarded during actual testing as they are not included in the testing.csv file within the /augmented-dataset/ directory\n","  dataToProcess = pd.read_csv(\"/content/selfissue-dataset-public-v1/testing-example-wav.csv\", header=None)\n","  for i, elem in dataToProcess.iterrows():\n","    audio1_path = os.path.join(\"/content/selfissue-dataset-public-v1/\", elem[0]) # elem[0] = played audio || elem[1] = recorded audio || elem[2] = label\n","    audio2_path = os.path.join(\"/content/selfissue-dataset-public-v1/\", elem[1])\n","    random.seed()\n","    mask1 = random.randrange(123) # so the upper limit is 122, in case it gets picked the 6px wide mask will hit rows 122, 123, 124, 125, 126 and 127.\n","    mask2 = random.randrange(123)\n","    preprocess(audio1_path, 1, mask1, mask2)\n","    preprocess(audio2_path, 2, mask1, mask2)"],"metadata":{"id":"zV5OBFtln7lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwxQm2dtRlj6"},"source":["# Dataset Class for our Siamese Network\n","\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import datetime\n","from pathlib import Path\n","from PIL import Image\n","from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","\n","class SiameseDataset():\n","    def __init__(self,training_csv=None,transform=None):\n","        self.train_df = pd.read_csv(base_path + \"\" + training_csv, header=None)\n","        self.transform = transform\n","\n","    def __getitem__(self,index):\n","        img1_path = os.path.join(base_path, self.train_df.iat[index,0])\n","        img1 = Image.open(img1_path)\n","        img1 = img1.convert(\"L\")\n","        img2_path = os.path.join(base_path, self.train_df.iat[index,1])\n","        img2 = Image.open(img2_path)\n","        img2 = img2.convert(\"L\")\n","        if self.transform is not None:\n","            img1 = self.transform(img1)\n","            img2 = self.transform(img2)\n","        return img1, img2, torch.from_numpy(np.array([int(self.train_df.iat[index,2])],dtype=np.float32)), img1_path, img2_path\n","        # it returns img1, img2, the label, and the related paths\n","\n","    def __len__(self):\n","        return len(self.train_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeXRsAI_UoGw"},"source":["# Structure of our Siamese Network\n","\n","class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","\n","        self.cnn1 = nn.Sequential(\n","            nn.Conv2d(1, 60, kernel_size=7, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(60),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(60, 48, kernel_size=7, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(48),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(48, 36, kernel_size=5, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(36),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(36, 24, kernel_size=5, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(24),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(24, 12, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(12),\n","            nn.Dropout2d(p=.25)\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(456, 300),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(300, 100),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(100, 20),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","    def forward_once(self, x):\n","        output = self.cnn1(x)\n","        output = output.view(output.size()[0], -1)\n","        output = self.fc1(output)\n","        return output\n","\n","    def forward(self, input1, input2):\n","        output1 = self.forward_once(input1)\n","        output2 = self.forward_once(input2)\n","        return output1, output2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZmh6RqcUxQH"},"source":["# Contrastive Loss, i.e. the criterion for our training\n","\n","class ContrastiveLoss(torch.nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, x0, x1, y):\n","        pairwise_distance = torch.nn.functional.pairwise_distance(x0, x1, keepdim=True)\n","        loss = torch.mean((1 - y) * torch.pow(pairwise_distance, 2) / 2  + (y) * torch.pow(torch.clamp(self.margin - pairwise_distance, min=0.0), 2) / 2)\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4B_GPSWNU1cH"},"source":["# Siamese Network Training\n","\n","from sklearn import metrics\n","import time\n","\n","# Initialize network, criterion and optimizer\n","net = SiameseNetwork().cuda()\n","criterion = ContrastiveLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.00005)\n","\n","# Define other variables for training\n","tot_epochs = 100\n","test_th = 0.4 # Threshold for the classifier\n","\n","# Load the training dataset\n","siamese_dataset = SiameseDataset(\"training.csv\", transform = transforms.Compose([transforms.Resize((128,650)), transforms.ToTensor()]))\n","train_dataloader = torch.utils.data.DataLoader(siamese_dataset,num_workers=1,batch_size=1,shuffle=True)\n","\n","# Load the validation dataset\n","test_dataset = SiameseDataset(\"testing.csv\", transform=transforms.Compose([transforms.Resize((128,650)), transforms.ToTensor()]))\n","test_dataloader = torch.utils.data.DataLoader(test_dataset,num_workers=1,batch_size=1,shuffle=True)\n","\n","# Training Function\n","def train():\n","    counter = []\n","    train_accuracy = []\n","    test_accuracy = []\n","    bestAcc = 0\n","    for epoch in range(1,tot_epochs+1):\n","        net.train() # Entering Training Mode\n","        for i, data in enumerate(train_dataloader,0):\n","            img0, img1, label, filename1, filename2 = data\n","            img0, img1, label = img0.cuda(), img1.cuda(), label.cuda()\n","            optimizer.zero_grad()\n","            output1,output2 = net(img0,img1)\n","            loss_contrastive = criterion(output1,output2,label)\n","            loss_contrastive.backward()\n","            optimizer.step()\n","        print(\"Epoch \"+str(epoch))\n","        counter.append(epoch) # X Axis of the Plot\n","        net.eval() # Exiting Training Mode, Entering Evaluation Mode\n","        vLoss, currentAccuracy = validate(train_dataloader, False, test_th) #Validation on training dataset\n","        train_accuracy.append(currentAccuracy) # Y Axis of the Plot (i)\n","        print(\"- Training Accuracy: \" + str(currentAccuracy))\n","        currentTestLoss, testAcc = validate(test_dataloader, False, test_th) # Validation on testing dataset\n","        test_accuracy.append(testAcc) # Y Axis of the Plot (ii)\n","        print(\"- Testing Accuracy: \" + str(testAcc))\n","        if (testAcc > bestAcc):\n","          bestAcc = testAcc\n","          torch.save(net.state_dict(), \"bestModel.pt\") # We save the best performing model so we can use it later\n","          print(\"Best Model Updated\")\n","        print(\"\")\n","    fig, (ax) = plt.subplots(1, 1, figsize=(10,5))\n","    ax.plot(counter, train_accuracy, 'r-', label='Train Accuracy')\n","    ax.plot(counter, test_accuracy, 'b-', label='Test Accuracy')\n","    ax.set_xlabel(\"Epoch\")\n","    ax.set_ylabel(\"Accuracy\")\n","    ax.set_ylim(ymin=0)\n","    ax.legend()\n","    return net # Returns the trained network after X epochs\n","\n","# Validation Function\n","def validate(dataloader, last, threshold):\n","  tp=0\n","  tn=0\n","  fp=0\n","  fn=0\n","  loss=0\n","  for i, data in enumerate(dataloader,0):\n","    x0, x1, label, fullname1, fullname2 = data\n","    output1,output2 = net(x0.to(device),x1.to(device))\n","    pdist = torch.nn.functional.pairwise_distance(output1, output2)\n","    if label==torch.FloatTensor([[0]]):\n","      label=\"Benign.\"\n","    else:\n","      label=\"Malicious!\"    \n","    prediction = \"Malicious!\" if pdist.item()>=threshold else \"Benign.\"\n","    if last:\n","      print(\"Now evaluating: \" + str(fullname1) + \" and \" + str(fullname2))\n","      print(\"Predicted Pairwise Distance: \", pdist.item())\n","      print(\"Prediction: \", prediction)\n","      print(\"Actual Label: \", label)\n","      print(\"\")\n","    loss = pdist.item()\n","    if prediction == label:\n","      if label == \"Malicious!\":\n","        tp = tp+1\n","      else:\n","        tn = tn+1\n","    else:\n","      if label == \"Malicious!\":\n","        fn = fn+1\n","      else:\n","        fp = fp+1\n","  accuracy = (tp+tn)/(tp+tn+fp+fn)\n","  if last:\n","    print(\"Accuracy: \" + str(accuracy))\n","  return loss, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now that we declared all variables and functions, it is time to train our network\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = train() # Training begins!\n","torch.save(model.state_dict(), \"model.pt\") # This is the latest model after X epochs of training. It is not necessarily the best one (almost always, it is not!). The best model is saved during training, and is called bestModel.pt\n","print(\"Latest Model Saved Successfully\")"],"metadata":{"id":"tAlm81-Wnw4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJ8M3LhzU3Bl"},"source":["# Let's see what the best model did, sample by sample\n","model_to_load = \"/content/bestModel.pt\" # We first pick the model we want to use\n","net = SiameseNetwork().cuda() # We initialize the network\n","net.load_state_dict(torch.load(model_to_load)) # We actually load the model\n","net.eval() # We enter evaluation mode\n","validate(test_dataloader, True, 0.4) # Evaluation starts!"],"execution_count":null,"outputs":[]}]}