{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyMrR4PvT3hioM8PZ74t6jTD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Single Played/Recorded Sample Pair Validation\n","\n","Given a *played* audio called `played_audio.wav` and a *recorded* audio called `rec_audio.wav`, this notebook extracts their Mel-Spectrograms and performs a prediction. Elaboration times are calculated for benchmarking purposes.\n","\n","### Instructions\n","\n","- All needed files are within the `monotest-samples.zip` file. This archive contains:\n","  - A pair of *played* and *recorded* audio files (`played_audio.wav` and `rec_audio.wav`)\n","  - The `monotest.csv` file. It is a dataset file that only contains one entry, that is, the Mel-Spectrograms generated from the `played_audio.wav`/`rec_audio.wav` audio pair. The label in this .csv file is ignored by this code.\n","  - The `111-th04-100acc.pt` file. It is a model that was pretrained with our solution, and that got 100% accuracy on the testing dataset, that is, it correctly classified all samples within that dataset. Please note that this model never saw the `played_audio.wav` and `rec_audio.wav` included in this archive.\n","- Just extract everything on your local machine and move all files in the same directory of this notebook. If you run this on Google Colab, you can put them in the `/content/` folder, that is, the directory already shown in the left panel when Google Colab is started.\n","\n","### Running this on Windows\n","It works as long as you install the following requirements (other versions of these packages are not tested, but they might work):\n","\n","- Librosa 0.9.2\n","- Matplotlib 3.6.0\n","- Numba 0.56.2\n","- Numpy 1.23.3\n","- Pandas 1.5.0\n","- Pillow 9.2.0\n","- Resampy 0.4.2\n","- Scikit-Image 0.19.3\n","- Scikit-Learn 1.1.2\n","- Torch 1.12.1\n","- Torchvision 0.13.1\n","\n","### Running this on Raspberry Pi 4 Model B (armv7l)\n","\n","Getting all the packages to work correctly on Raspberry Pi was quite tricky. Here are some brief instructions on how to do so:\n","\n","1. sudo apt-get install llvm\n","2. LLVM_CONFIG=/usr/bin/llvm-config sudo pip3 install llvmlite==0.32.1\n","3. sudo apt-get install libatlas3-base\n","4. sudo pip3 install numba==0.49\n","5. sudo pip3 install librosa==0.8.0\n","6. Install the remaining packages in any order\n","\n","- Matplotlib 3.0.2\n","- Numpy 1.21.6\n","- Pandas 1.3.5\n","- Pillow 9.2.0\n","- Resampy 0.2.2\n","- Scikit-Image 0.19.3\n","- Scikit-Learn 1.0.2\n","- Torch 1.7.0 (from https://github.com/Kashu7100/pytorch-armv7l )\n","- Torchvision 0.8.0 (from https://github.com/Kashu7100/pytorch-armv7l )\n","\n","Please install the exact versions stated above if you want to run this solution on Raspberry Pi, or you will probably encounter several problems during the installation process or during the execution of this notebook."],"metadata":{"id":"fQYI026rIOXc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1ZIL58tHzp-"},"outputs":[],"source":["import librosa\n","import numpy as np\n","import pandas as pd\n","import os\n","import skimage.io\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","import time\n","\n","base_path = \"\"\n","played_audio_path = \"played_audio.wav\"\n","rec_audio_path = \"rec_audio.wav\"\n","\n","def scale_minmax(X, min=0.0, max=1.0):\n","  X_std = (X - X.min()) / (X.max() - X.min())\n","  X_scaled = X_std * (max - min) + min\n","  return X_scaled\n","\n","def trim(img, offset=0):\n","  return img[:,0+offset:]\n","\n","def preprocess(audiofile, num):\n","  y, sr = librosa.load(audiofile, sr=None)\n","  img = generate_mel_spectrogram(y, sr, num)\n","  if (num == 1):\n","    finalName = \"played_audio_melspec.png\"\n","  else:\n","    finalName = \"rec_audio_melspec.png\"\n","  print(\"Saving: \" + finalName)\n","  skimage.io.imsave(finalName, img)\n","\n","def generate_mel_spectrogram(y, sr, num):\n","  mel = librosa.feature.melspectrogram(y=y, sr=sr)\n","  mel_db = librosa.power_to_db(mel, ref=np.max)\n","  img = scale_minmax(mel_db, 0, 255).astype(np.uint8)\n","  img = np.flip(img, axis=0)\n","  img = 255-img\n","  if (num == 1):\n","    img = trim(img, 0)\n","  else:\n","    img = trim(img, 20)\n","  return img\n","\n","if __name__ ==  '__main__':\n","  ppStartTime = time.time()\n","  preprocess(played_audio_path, 1)\n","  preprocess(rec_audio_path, 2)\n","  print(\"Preprocessing done in \" + str(time.time() - ppStartTime) + \" seconds.\")\n","  print(\"\")\n","\n","# - - - - - - - - - DATASET DESCRIPTION - - - - - - - - - - #\n","\n","class SiameseDataset():\n","    def __init__(self,training_csv=None,transform=None):\n","        self.train_df = pd.read_csv(base_path + \"\" + training_csv, header=None)\n","        self.transform = transform\n","\n","    def __getitem__(self,index):\n","        img1_path = os.path.join(base_path, self.train_df.iat[index,0])\n","        img1 = Image.open(img1_path)\n","        img1 = img1.convert(\"L\")\n","        img2_path = os.path.join(base_path, self.train_df.iat[index,1])\n","        img2 = Image.open(img2_path)\n","        img2 = img2.convert(\"L\")\n","\n","        if self.transform is not None:\n","            img1 = self.transform(img1)\n","            img2 = self.transform(img2)\n","        return img1, img2, torch.from_numpy(np.array([int(self.train_df.iat[index,2])],dtype=np.float32)), img1_path, img2_path\n","\n","    def __len__(self):\n","        return len(self.train_df)\n","\n","# - - - - - - - - - NEURAL NETWORK DESCRIPTION - - - - - - - - - - #\n","    \n","class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","\n","        self.cnn1 = nn.Sequential(\n","            nn.Conv2d(1, 60, kernel_size=7, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(60),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(60, 48, kernel_size=7, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(48),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(48, 36, kernel_size=5, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(36),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(36, 24, kernel_size=5, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(24),\n","            nn.Dropout2d(p=.25),\n","\n","            nn.Conv2d(24, 12, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(12),\n","            nn.Dropout2d(p=.25)\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(456, 300),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(300, 100),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Linear(100, 20),\n","            nn.ReLU(inplace=True)\n","        )\n","        \n","    def forward_once(self, x):\n","        output = self.cnn1(x)\n","        output = output.view(output.size()[0], -1)\n","        output = self.fc1(output)\n","        return output\n","\n","    def forward(self, input1, input2):\n","        output1 = self.forward_once(input1)\n","        output2 = self.forward_once(input2)\n","        return output1, output2\n","\n","# - - - - - - - - - VALIDATION - - - - - - - - - - #\n","\n","def validate(dataloader, last, threshold, network):\n","  for i, data in enumerate(dataloader,0):\n","    x0, x1, label, fullname1, fullname2 = data\n","    output1,output2 = network(x0.to(device),x1.to(device))\n","    pdist = torch.nn.functional.pairwise_distance(output1, output2)\n","    if label==torch.FloatTensor([[0]]):\n","      label=\"Benign.\"\n","    else:\n","      label=\"Malicious!\"\n","    prediction = \"Malicious!\" if pdist.item()>=threshold else \"Benign.\"\n","    print(\"Now evaluating: \" + str(fullname1) + \" and \" + str(fullname2))\n","    print(\"Predicted Pairwise Distance: \", pdist.item())\n","    print(\"Prediction: \", prediction)\n","    print(\"\") \n","\n","if __name__ ==  '__main__':\n","  setupStartTime = time.time()\n","  net = SiameseNetwork()\n","  test_th = 0.4\n","  test_dataset = SiameseDataset(\"monotest.csv\", transform=transforms.Compose([transforms.Resize((128,650)), transforms.ToTensor()])) #The label in this csv file is not used\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset,num_workers=1,batch_size=1,shuffle=True)\n","  device = torch.device('cpu')\n","  model_to_load = \"111-th04-100acc.pt\"\n","  net.load_state_dict(torch.load(model_to_load, map_location=torch.device('cpu')))\n","  net.eval()\n","  print(\"Network initialized in \" + str(time.time() - setupStartTime) + \" seconds.\")\n","  print(\"\")\n","  \n","  validationStartTime = time.time()\n","  validate(test_dataloader, True, test_th, net)\n","  print(\"Prediction elaborated in \" + str(time.time() - validationStartTime) + \" seconds.\")\n","  "]}]}